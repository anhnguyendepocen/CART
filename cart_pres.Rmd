---
title: "Classification and Regression Trees"
author: "Clay Ford"
date: "Spring 2015"
output: beamer_presentation
---

## In this workshop

- What are and how to read classification and regression trees 
- How to create classification and regression trees in R
- Overview of how trees are constructed
- How to interpret output from building trees
- How to improve prediction accuracy of trees
- How trees compare to other modeling approaches

## What are Classification and Regression Trees?

They are basically _decision trees_ that make predictions based on binary splits. 

**Classification trees** predict a classification (categorical response). 

**Regression trees** predict means or medians (continuous response).

They are non-parametric in nature in that they don't make distributional assumptions about the response or prediction errors.

## Example of Classification Tree

![classification tree example](ctree_ex.png)

## Example of Regression Tree

![Regression tree example](rtree_ex.png)

## How to read the trees

Start at the top of the tree, called the _root node_:

- go left if condition is true  
- otherwise go right  

Repeat process at subsequent _nodes_ until reaching a _terminal node_, or _leaf_. The value in the leaf is the predicted value.

Hence classification and regression trees tend to be easier to interpret than traditional linear modeling output. 

However, that does not mean they always "perform" better than linear models. 

## Using R to build trees

There are several packages available for building decision trees in R.

The package we'll use in this workshop is `rpart`, which is short for "Recursive Partitioning".

The main function is `rpart`. The basic steps to fitting and plotting a tree are:

1. `fit <- rpart(response ~ x1 + x2 + x3..., data=DF)` where DF is your data frame and x1, x2, ... are your predictors.
2. `plot(fit)`
3. `text(fit)`

If `response` is a factor, then a classification tree is built. Otherwise a regression tree is built.

## Plotting tree branches

Calling `plot` on an rpart object produces the tree without labels. Additional arguments of note include

- `uniform`: is vertical spacing between nodes uniform? The default is FALSE. 

- `branch`: number between 0 and 1 that controls the shape of branches. 1 draws square-shouldered branches, 0 draws V-shaped branches, and values in between draw 
a combination. Default is 1.

- `compress`: should routine attempt a more compact arrangement of tree? Default is FALSE.

- `margin`: extra fraction of white space to leave around the borders of the tree. Default is 0.


## Adding text to tree branches

Calling `text` on an rpart object adds text labels to tree branches. Additional arguments of note include

- `use.n`: show number of observations in leaves? Default is FALSE.

- `all`: label all nodes with additional information? Default is FALSE. 

- `cex`: character expansion, expressed as a fraction of 1 (normal size). Greater than 1, bigger text; less than 1, smaller text.

Let's go to R!

## How trees are constructed

The basic process:

1. Examine all possible splits for all possible covariates and choose the split which leads to 2 groups that are "purer" than the current group. That is, take the split that produces the largest improvement in _purity_.
2. Repeat step 1 for the 2 new groups. Hence the name _recursive partitioning_.
3. Repeat steps 1 and 2 for all subsequent groups until some stopping criterion is fulfilled. 

In the `rpart` package, a node must have at least 20 observations for it to attempt a split. This is a setting that can be modified.

## A 2-dimensional example of classification tree

![tree with plot](tree_and_plot.png)

## Measures of impurity - Classification Trees

For classification trees, the usual measure of impurity is the _Gini index_. We select a split such that the Gini index is minimized:

$$ G = \sum^K_{k=1}\hat{p}_{mk}(1-\hat{p}_{mk})$$

where $K$ is the number of classes and $\hat{p}_{mk}$ is the proportion of observations in the _m_th group that are from the _k_th class.

## Why use the Gini index?

Simply put, it's more _sensitive_ than misclassification rate:
```{r, echo=FALSE}
pm1 <- seq(0,1,0.01)
pm2 <- 1 - pm1
class <- 1 - pmax(pm1,pm2)
gini <- pm1*(1-pm1) + pm2*(1-pm2)
plot(pm1, class, type="l", xlab="Proportion belonging to class 1", ylab="impurity", 
     ylim=c(0,0.6), main=("Gini vs. Misclassification\nfor two-class response"))
lines(pm1, gini, type="l", col="red")
legend("topright", legend = c("Gini Index", "Misclassification Error"), 
       col = c("red","black"),lty = 1)
```

## Gini index vs misclassification rate example

Let's say we have a two-class response with 400 observations in each class (400, 400).

Suppose when growing a tree we have two splits to consider:

1. nodes with (300, 100) and (100, 300)    
2. nodes with (200, 400) and (200, 0)    

Both splits produce a misclassification rate of $200/800 = 0.25$. There is no preferred split.

But the second split produces a pure node and is probably preferable. The Gini index captures this:   
$(300/400)\times(100/400) + (300/400)\times(100/400) = 0.375$   
$(200/600)\times(400/600) + (200/200)\times(0/200) = 0.222$   
The second split has nodes with lower _impurity_.


## Measuring improvement in purity - Classification Trees

The improvement in purity as displayed in `rpart` is calculated as follows:

$$\Delta I = n[p(A)I(A) - p(A_{L})I(A_{L}) -  p(A_{R})I(A_{R})] $$

where $p(A)$ is the proportion of observations in the current node; $p(A_{L})$ and $p(A_{R})$ are the proportion of observations sent to the left and right child nodes, respectively; $I(A)$, $I(A_{L})$ and $I(A_{L})$ are the measures of impurity in each node; and $n$ is the total number of observations.

The actual values of improvement are not that important but their relative size gives an indication of the utility of the variables.

## Measuring improvement in purity - example cont'd

Let's say we have a two-class response with 400 observations in each class (400/400).

Suppose when growing a tree we have two splits to consider:

1. nodes with 300/100 and 100/300    
2. nodes with 200/400 and 200/0 

The second has the largest improvement in purity:

$800[(0.5)(0.5) - (0.5)(300/400)(100/400) - (0.5)(300/400)(100/400)] = 50$

$800[(0.5)(0.5) - (0.75)(200/600)(400/600) - (0.25)(200/200)(0/200)] = 66.66667$


       
## Measures of impurity - Regression Trees

For regression trees, the usual measure of purity is the Residual Sum of Squares (RSS). We select a split such that the RSS is minimized:

$$RSS = \sum^J_{j=1} \sum_{i \in R_{j}} (y_i - \hat{y}_{R_{j}})^2$$

where $J$ is the number of groups, $R_{j}$ are the _regions_ of predictors, and $\hat{y}_{R_{j}}$ is the mean repsonse for the _Jth_ group.



## Measuring improvement in purity - Regression Trees

The improvement in purity as displayed in `rpart` is calculated as follows:

$$\Delta I = 1 - (SS_{right} + SS_{left})/SS_{parent}$$

where $SS$ is the respective sum of squares for the parent and children nodes: $\sum (y_{i} - \bar{y})^2$.

## The summary of tree construction

A summary of the tree construction process can be viewed with the `summary` function. Beware, it produces a lot of output!

It displays the _primary_ split for each node as well as the four "runner-up" splits (the splits that came closest to the chosen split in terms of increasing purity).

It also displays five _surrogate_ splits. These are splits for observations that are missing data for the primary split.

Any observation missing the primary split variable is classified using the first surrogate split, or if missing that, the second surrogate, and so on.

Let's go to R.

## Pruning trees

Once we grow a tree, we usually need to _prune_ it. That is, we need to trim off the bottom branches.

A tree that is too large can overfit the data and not perform well for data not used in growing the tree.

On the other hand, a tree that is too small will not include important predictors and also not perform well.

Therefore pruning a tree can pose a challenge.

## A Large Tree - overfits data

![Big tree example](tree_big.png)

## A Small Tree - underfits data

![Big tree example](tree_small.png)

## How pruning works

Pruning is basically variable selection. We build a large tree and then _prune_ branches that don't significantly improve performance. 

The "significance" in pruning is achieved by minimizing the following cost function:

$$R_{\alpha}(T) = R(T) + \alpha|T| $$

where $\alpha$ is the complexity parameter ranging from 0 to $\infty$, $R(T)$ is the _risk_ of the tree (ie, the proportion misclassified), and $|T|$ is the number of terminal nodes (leaves) in a tree. Thus $R_{\alpha}(T)$ is the risk of a tree for a given $\alpha$.

We build a large tree, minimize $R_{\alpha}(T)$ over several values of $\alpha$ to obtain a sequence of pruned trees, and then select the pruned tree with the smallest risk.

## The complexity parameter, $\alpha$

$\alpha$ measures the _cost_ of adding another variable to the tree. In other words, there's a penalty to pay for adding more branches to a tree. $\alpha$ is the price you pay to add more branches.

$$R_{\alpha}(T) = R(T) + \alpha|T| $$

If $\alpha$ is small, the penalty is small and the tree will be large. The minimal value is achieved by having lower rates of misclassifications.

If $\alpha$ is large, the penalty is large and the tree will be small. The minimal value is achieved by having fewer terminal nodes (leaves).

## Selecting the optimal $\alpha$

We use K-fold cross validation to help us find the optimal $\alpha$. That is, we divide our data into K folds (or sets), and then for each K:

1. Hold out set K and grow a large tree with the other folds  
2. Obtain a sequence of best subtrees as a function of $\alpha$  
3. Evaluate the error for the left out set K (misclassification rate for classification trees; mean squared prediction error for regression trees)   
4. Average the results for each value of $\alpha$ and pick $\alpha$ to minimize the error.

Fortunately, `rpart` does this for us with a default of K = 10.

## Pruning in `rpart`

The pruning results can be viewed with the `printcp` function.

- `CP`: the complexity parameter $\alpha$ scaled by the number of misclassifications in a model with no splits.  
- `nsplit`: number of splits (number of terminal nodes is `1 + nsplit`)  
- `rel error`: relative error (calculated with same data used to build tree)
- `xerror`: cross-validated error rate (calculated with held out data)  
- `xstd`:  cross-validated standard error

Use the `plotcp` function to plot `xerror` vs. `cp`.

Let's go to R.

## Making predictions

Use the `predict` function to run data through your tree and make predictions.

For classification trees there are three types of predictions you can make:  

- `predict(tree, type="prob")`: predicted probabilities (default)    
- `predict(tree, type="class")`: predicted class using level label  
- `predict(tree, type="vector")`: predicted class using level number    

For regression trees, calling `predict(tree)` returns predicted means.

## Making predictions with data not used to build tree

To make predictions for data not used to build tree, use the `newdata=` argument. 

The new data must be a data frame and include all variables used in building the tree _even if the tree only includes a subset of the variables_.

Let's go to R.


## Improving prediction accuracy of trees

Classification and Regression trees tend to suffer from _high variance_. That is they tend to produce different sized trees over multiple fittings.

As we'll demonstrate in the R script, pruning with cross validation does not always result in pruning the same size tree.

Two approaches to reducing this variability is _bagging_ and _random forests_.


## Bagging

"Boostrap aggregation", or _Bagging_, means resampling our data B times, building B trees, and then averaging all the predictions. 

"Resampling" means sampling _with replacement_ from our original data until we have a data set the same size as our original data. 

Two keys:  

1. We do not prune our trees. We grow them deep and use them to make predictions.  
2. We make predictions using observations that were not used to grow the tree. These are called _out-of-bag_ (OOB) predictions.  

For regression trees, the average is simply the mean of the B predicted means. For classification trees, the prediction is the most commonly occuring prediction (ie, _majority vote_)


## Random Forests

Random forests are similar to bagging, except _we only consider a random sample of $m$ predictors at each split_.

A fresh sample of predictors is taken at each split.

We usually use $m \approx \sqrt{p}$, that is the number of predictors at each step is about equal to the square root of the total number of predictors. When $m = p$, we have bagging.

Sampling predictors at each split prevents strong predictors from dominating the tops of trees and gives moderately-strong predictors a chance to perform. Think of this as allowing some of the "runner-ups" in the summary results to have a turn as a primary split.


## Implementation of bagging and random forests

The `randomForest` package allows you to easily implement bagging and random forests. The syntax is very similar to `rpart`.

**Bagging**:   
`bag.tree <- randomForest(response ~ ., data=DF, mtry=p)`

**Random forest**:   
`rf.tree <- randomForest(response ~ .,data=DF)`

The `mtry=` argument specifies how many predictors to sample at each split. Recall $m = p$ is bagging. The default for classification is $\sqrt{p}$ while the default for regression is $p/3$.


## The drawback of bagging and random forests

While these two approaches reduce variability and improve performance, they sacrifice the ease of interpretability.

Instead of one tree we can easily interpret, we now have many trees (usually 500) that we are using as an _ensemble_.

One way around this is to create a plot of _variable importance_. 

This plot shows the most important predictors based on the total _decrease in node impurity_ that results from splits over the variable.

## Example of variable importance plot

![variable importance plot](varImport.png)

Let's go to R.

## References

A Handbook of Statistical Analyses using R (Chapter 8)

An Introduction to Statistical Learning with Applications in R (Chapter 8)

The Elements of Statistical Learning (pp. 308 - 310)

An Introduction to Recursive Partitioning Using the RPART Routines